---
author: "Landon Batts and Jose Singer-Freeman"
date: "due 2023-07-09"
#output: github_document
params: 
      channel: ""
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE,  warning=FALSE)
#if currently blocked, unblock the channel parameter

if (bindingIsLocked("params", env = .GlobalEnv)==TRUE) {
      unlockBinding("params", env = .GlobalEnv)}

```


```{r  automation}

# Create chosenChannel variable to used for title and other inline code
# create separate file paths for images and cached files so that they are not ovewritten 

if(params$channel=="data_channel_is_lifestyle"){
  chosenChannel="lifestyle"
  knitr::opts_chunk$set(fig.path = "images/lifestyle/",
                        cache.path = "cache/lifestyle/")
} else if(params$channel=="data_channel_is_entertainment"){
    chosenChannel="Entertainment"
    knitr::opts_chunk$set(fig.path = "images/entert/",
                        cache.path = "cache/entert/")
}  else if(params$channel=="data_channel_is_bus"){
      chosenChannel="Business"
      knitr::opts_chunk$set(fig.path = "images/bus/",
                        cache.path = "cache/bus/")
}   else if(params$channel=="data_channel_is_socmed"){
    chosenChannel="Social Media"
    knitr::opts_chunk$set(fig.path = "images/socmed/",
                        cache.path = "cache/socmed/")
} else if(params$channel=="data_channel_is_tech"){
  chosenChannel="Technology"
  knitr::opts_chunk$set(fig.path = "images/tech/",
                        cache.path = "cache/tech/")
} else if (params$channel=="data_channel_is_world"){
  chosenChannel="World"
  knitr::opts_chunk$set(fig.path = "images/world/",
                        cache.path = "cache/world/")
  }        
     

```


---
title: "Analysis of the `r chosenChannel` Channel"
subtitle: "Online News Popularity"
---

```{r load packages, include=FALSE}
library(tidyverse)
library(caret)
library(leaps)
library(doParallel)
library(dplyr)
library(corrplot)
library(knitr)
library(DescTools)
library(reshape2)
library(MASS)
library(tidyr)
  
```
# Online News Popularity Analysis for `r  chosenChannel` Channel

## 1.Introduction


In this project we work with the [online news popularity data set](https://archive.ics.uci.edu/dataset/332/online+news+popularity)  published by UC Irvine.  The data set summarizes a heterogeneous set of features about articles published by Mashable in a period of two years.  

Our goal is to predict the number of "shares" (popularity) of articles in social networks.  This is done separately for 6 different types or "channels" of articles, namely, lifestyle, entertainment, business, social media, tech and world news. This is the analysis for the `r  chosenChannel`.  You can find the analyses for the other channels in the README file.      

Our predictive models consist of two linear regression models, a random forest model and a boosted tree model for each of the 6 channels. 


##  2.  Import Data


```{r import and wrangle data}
#Get data

raw_data<-read.csv("~/ST558/Project2/OnlineNewsPopularity.csv")

# Remove variables we won't use
newsData<-raw_data %>% dplyr::select(-c(url,timedelta))

#Filter data from a single channel ( as a test before automation) using the parameter "params$channel"

#store business channel the global channel parameter 
#params$channel<-"data_channel_is_bus"

#Reduce the number of observations to 1000 - trial run
newsData<-newsData[3000:4000,]

# convert the parameter into a name and then select the rows  where the channel is 1
channelData<-newsData %>% filter(eval(as.name(params$channel))==1)
#shares imported as integer.  Convert to double for purposes of analysis.. 
chanellData<-channelData%>%mutate(shares=as.numeric(shares))

rm(newsdata)

```


##  3. Basic Summary Statistics



### Summary Statistics

The table below shows pairs of predictor variables whose correlation, in absolute terms, is above 0.75.  For those that exceed 0.75, we will drop a member of each pair of variables from our models.  This is a form of feature selection. 

```{r correlation}
#Correlation

#1. Choose predictors that are highly correlated with Shares
tempordata<-channelData[-c(12:17)] #remove channel variables

corMat<-cor(tempordata, use="pairwise.complete.obs")

#get correlation of "shares" with all other variables
  
share_cor <- data.frame("r"=corMat["shares",])  #chose all correlations with shares 

share_cor<-share_cor%>%   
  rownames_to_column("variable")%>%  #convert the rownames into a column
  filter(!variable=="shares")%>%  #exclude row of shares 
  arrange(desc(r))


# 2. Choose predictors that are the most highly correlated among themselves.  First drop higher triangle to avoid duplicates and then remove the diagonal. 

corMat[lower.tri(corMat,diag=TRUE)] <- NA  # drop upper triangle
corMat[corMat == 1] <- NA  #drop perfect correlations

corMat <- as.data.frame(as.table(corMat)) #form a  table with 3 columns: 2-variables and their correlation
corMat <- na.omit(corMat) #remove missing values

corMat<-subset(corMat, abs(Freq) > 0.75) #select correlation values above 0.5  
corMat <- corMat[order(-abs(corMat$Freq)),] #sort by highest to lowest correlation



#choose variables with correlation above 0.75, in order to exclude them from certain model
droppedVars<-corMat%>%
   dplyr::select(Var2)


#turn correlation back into matrix in order to plot with corrplot
  corMat2 <- reshape2::acast(corMat, Var1~Var2, value.var="Freq") #melt the data

  
  #Print table and chart for correlations among predictors

  knitr::kable(corMat, col.names=as.vector(c("Variable 1", "Variable 2", "Correlation")),  digits = 2, booktabs = TRUE)
    corrplot(corMat2, is.corr=FALSE, tl.col="black", na.label=" ") #plot  correlations absolute value above 0.5

    

```

For two of our models, we will only use predictors that are highly correlated with shares.  The table below shows 20 variables with highest correlation to the response, shares.  We excluded those predictors that were highlighted in the prior section, i.e., those with high correlation with another predictor. 

```{r high correlated variables}
#20 variables with highest correlation to shares (excluding one member of pairs of predictors that are highly correlated among themselves)

highcor<-share_cor%>%
  filter(!variable %in% droppedVars)%>% # Remove those that are highly correlated among themselves
  slice(1:20) #chose top 20

knitr::kable(highcor, digits=3, col.names=c("variable", "correlation with shares"))

```

The following shows a scatterplot between shares and each of the the  3 predictors with which it is mostly highly correlated.

```{r top3 correlation plot}
#draw scatterplot of number of shares v. each of the three top 3 highly correlated (that are non-binary)


# Select top 3 predictors that are not binary and are highly correlated with shares
top_3<-share_cor%>%
  dplyr::select(-matches("is_"))%>%  #remove binary variables
  slice(1:3)%>%  #choose top 3 correlated predictors
  dplyr::select(variable)%>%  #choose the variable column
  unlist()%>%  #convert into a vector of variable names
  unname()  

#Plot 

 channelData %>%
   dplyr::select(shares, top_3)%>%
   mutate(shares=as.numeric(shares))%>%
  tidyr::pivot_longer(top_3, names_to = "top_3", values_to = "value")%>%
   ggplot(aes(x = value, y=shares, color=top_3)) +
  geom_point() +
  facet_wrap(vars(top_3), ncol = 3) +
  labs(x = "Top 3 predictors", y = "shares")+ guides(color="none")+ 
   scale_y_continuous(labels = scales::comma)+ # remove scientific notation for axis labels
   scale_x_continuous(labels = scales::comma)

```


```{r weekday summary}
#weekday-shares relationship

#  Calculate average shares per week day. First combine weekday variables into one new variable to ease calculation.  
dayData<-channelData%>%
  dplyr::select(shares,starts_with("weekday_is"))%>%
    mutate(day_of_week=factor(case_when(as.logical(weekday_is_monday)~"Monday",
                             as.logical(weekday_is_tuesday)~"Tuesday",
                             as.logical(weekday_is_wednesday)~"Wednesday",
                             as.logical(weekday_is_thursday)~"Thursday",
                             as.logical(weekday_is_friday)~"Friday",
                             as.logical(weekday_is_saturday)~"Saturday",
                             as.logical(weekday_is_sunday)~"Sunday"),ordered=TRUE,levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday")))%>%
    dplyr::select(-starts_with("weekday_is"))%>%
  group_by(day_of_week)%>%
    summarize(avg_Shares=mean(shares, na.rm=TRUE), num_articles=n())

# Average Number of Shares by Day of Week 

# The first bar plot below shows the average number of shares by day of the week.  If weekends show higher shares, it may mean that people have more time in their hands to read the articles.  If weekdays are more popular for sharing, it may have to do with the subject matter of the articles or the number of published articles. 
#The second bar plot below shows the number of articles published by day of week.  By comparing it to the first plot, you may glean whether a higher average number shares in a week day can be explained by the availability of more  articles on that day. 


g1<-ggplot(dayData, aes(x = day_of_week)) 
 
par(mfrow=c(1,2))
g1+geom_bar(stat="identity",color = "lightgreen", aes(y = avg_Shares, fill = day_of_week) )+  scale_fill_grey()+ guides(fill="none")+labs(title="Average Shares by Day of Week", x="Day of Week", y="Avg. Shares")

g1+geom_bar(stat="identity",color = "lightgreen", aes(y = num_articles, fill = day_of_week) )+  scale_fill_grey()+ guides(fill="none")+labs(title="Number of Articles by Day of Week", x="Day of Week", y="Articles")

```

### Plots
```{r plots and summaries}
#We can inspect the trend of the amount of shares as a function of the number of word in the content itself. If the points show an upward trend,then longer articles are shared more often. On the other hand, if there is a negative trend, then shorter articles are shared more.
g <- ggplot(channelData, aes(x=n_tokens_content,y=shares))
g + geom_point() + geom_smooth() +
  labs(title="Word Count vs Shares",x="Number of Words in Content", y="Amount of Shares")
```

```{r}


#In order to get an idea of the number of shares we are dealing with, a 5 number summary can be created to get an idea of the distribution, along with standard deviation to get an idea of the spread
summarise(channelData,min=min(shares),
          Q1=quantile(shares,0.25),
          med=quantile(shares,0.5),
          mean=mean(shares),
          sd=sd(shares),
          Q3=quantile(shares,.75),
          max=max(shares)
          )

#To visualize this, a box plot can be created to visualize the spread and the 5 number summary 
g2 <- ggplot(channelData,aes(x=shares))
g2 + geom_boxplot() + xlim(0,5000) + 
  labs(title="Shares Boxplot",x="Amount of Shares")
```


##  4. Modeling

We first split the data for each channel into 70% for testing and 30% for training. 
```{r split data}
#use set.seed for reproducibility
set.seed(101)

trainIndex <-caret::createDataPartition(channelData$shares, p = 0.7, list = FALSE)

training <-channelData[trainIndex,]
testing <-channelData[-trainIndex,]

```

For all models we will use 10-fold cross-validation repeated 5 times for each type of model.  Using the caret package, we set up the "control" that provides for this cross-validation. 

```{r control}
#We will use repeated 10-fold cv for all predictive techniques 

controlObject<-trainControl(method = "repeatedcv", 
                        number = 10, 
                        repeats=5)
```

```{r remove zero variance}
#remove predictors with near zero variance (including indicators for other channels)

# check how much variance there is for all variables 
#nzv <- nearZeroVar(channelData, saveMetrics= TRUE)

#removed 8 variables with low variance

# Get variables with zero or near-zero variance and remove them from the dataframe
nzv <- nearZeroVar(channelData)
filteredChannelData <- channelData[, -nzv]

```



### Linear Regression Models
Linear regression models are a type of statistical analysis in which a formula is created and used to predict the relationship between variables. It assumes a straight, linear relationship between variables and attempts to find a line of best fit in order to relate them. Here, we will perform linear regressions in which we try to predict shares using either  all other the variables or a subset of them as the predictors.

####  Linear Regression Model 1

Performed a simple linear regression with all variables except those with zero variance. Also, preprocessed the data by centering and scaling.

``` {r linear regression model 1}
set.seed(102)
 lm1Fit <- train(shares~., data=filteredChannelData,
                 method="lm",
                 preProcess=c("center","scale"),
                 trControl=controlObject)
 lm1Fit
 predictionlm1 <- predict(lm1Fit,newdata = testing)
 
 # #Get RMSE value
 
 lm1_RMSE<-data.frame(model="Linear Regression 1", 
                      RMSE=postResample(predictionlm1, obs = testing$shares)["RMSE"][[1]])


```

####  Linear Regression Model 2

Performed linear regression with  20 predictors with highest correlation with shares.  Excluded  predictors that may cause multicollineality.  Also, excluded variables that have near zero variance.  For preprocessing, we standardized the predictors and applied Box Cox transformation. 

```{r linear regression model 2}
#Note: Need to remove one day of week and 

set.seed(102)

#Build model with 10 highest (pairwise) correlated  predictors with shares, excluding those with near zero variance.


#identify variables that are in the  near zero variance list

nearZerovars<-colnames(channelData[,nzv])

#save the names of the 10 predictors with highest correlation as a row vector in order to  use later for a linear model


highcor<-share_cor%>%
  filter(!variable %in% nearZerovars & !variable %in% droppedVars)%>% # Remove vars that have near zero variance and remove vars that are highly correlated among themselves
  slice(1:20)

highcor2<-as.character(unname(unlist(highcor[1]))) #shape variables into a character row vector


RHSformu<-paste(highcor2, collapse="+")  #use the droppedVars variables to form the right hand side of the formula (separated by + signs)
formu<-as.formula(paste("shares~",RHSformu))  #add the left hand side of the formula and do a quadradic polynomial of the right hand side.


 lm2Fit<-train(formu,
            data = channelData,
            method="lm",
            preProcess =c("center","scale"),
            trControl= controlObject,
            verbose=FALSE
            )
 
 predictionlm2 <- predict(lm2Fit,newdata = testing)


#Get RMSE value
lm2_RMSE<-data.frame(model="Linear Regression 2", 
                     RMSE=postResample(predictionlm2, obs = testing$shares)["RMSE"][[1]])
```


###  Random Forest Model
The random forest model is an extension of the idea of bootstrap aggregation in which multiple trees are created from bootstrap samples and results are averaged. The difference is that only a random subset of the predictors for each sample/fit.

```{r rf model}

 #cl <- makePSOCKcluster(5)  #start parallel computing with 5 cores
 #registerDoParallel(cl)
# 
# set.seed(102)
# randomforestFit <- train(shares~.,data=filteredChannelData,
#                method="rf",
#                trControl=controlObject,
#                tuneGrid=data.frame(mtry=1:10))
# rfFit$results
# rfFit$bestTune
# predictionrf <- predict(rfFit,newdata = testing)
# RF_RMSE<-data.frame(model="Random Forest",RMSE=postResample(predictionrf, obs = filteredChannelData$shares))
```

###  Boosted Tree Model


The boosted tree method is an ensemble learning technique that combines decision trees. The algorithm starts by training an initial tree (a weak learner) on the training data. It then assigns weights to the training instances based on their difficulty of classification. In each iteration, the algorithm constructs a new tree  trained on the same dataset, with adjusted weights to focus on the misclassified instances from the previous iterations.  The predictions of all the trees are combined using a weighted sum or a voting scheme.

The tuning parameters are the typically the maximum tree depth, number of trees, and a shrinkage parameter (learning rate).

```{r boost tree}

 BTformula<-formu  #use same formula as for the second linear regression model 
 
  set.seed(122)
  
  tunegridBT<-expand.grid(n.trees=c(25, 50, 100, 150, 200), 
                             interaction.depth=1:4, 
                             shrinkage=0.1, 
                             n.minobsinnode=10)

  BTfit <- train(
    form = BTformula,
    data = filteredChannelData,
    trControl= controlObject,
    method="gbm",
    tuneGrid=tunegridBT,
    #preProcess =c("center","scale"),
    na.action = na.omit,
    verbose = FALSE
    #trace=0
      )
 
 #stopCluster(cl) #end parallel computing 
 
  predictionBT <- predict(BTfit,newdata = testing)
  BT_RMSE<-data.frame(model="Boosted Tree", RMSE=postResample(predictionBT, obs = filteredChannelData$shares)["RMSE"][[1]])


```

## Model Comparisons

We calculated the root mean square error (RMSE) for each model using the  testing data, shown in the table below. The winning model will be the model with the lowest RMSE.

```{r model comparison}
#initialize dataframe to contain RMSEs
perfMetric<-data.frame() 
#Add the RMSEs as rows 
perfMetric<-rbind(lm1_RMSE,
            lm2_RMSE,
            #RF_RMSE,
            BT_RMSE)

kable(perfMetric)

bestModel<-perfMetric[which.min(perfMetric$RMSE),][1]

           

```

The winning model is: **`r bestModel`**.



